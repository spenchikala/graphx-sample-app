
---------------------------------------------------------------------------------------------------------
//
// Reference:
// http://spark.apache.org/docs/latest/graphx-programming-guide.html
//
---------------------------------------------------------------------------------------------------------
//
//  Launch Spark Scala Shell:
//

c:
set JAVA_HOME=c:\dev\java\jdk1.6.0_45
set PATH=%PATH%;%JAVA_HOME%\bin

set SCALA_HOME=C:\dev\languages\scala
set PATH=%PATH%;%SCALA_HOME%\bin

REM set SPARK_HOME=c:\dev\servers\spark-1.0.1-bin-hadoop2
set SPARK_HOME=c:\dev\servers\spark-1.0.2-bin-hadoop2
set PATH=%PATH%;%SPARK_HOME%\bin

cd c:\dev\projects\BigDataBootCampLabs\SparkWorkshop\SparkWorkshopLabs\graphx-sample-app

REM  GraphX requires the Kryo serializer to achieve maximum performance. 
REM  To see what serializer is being used check the Spark Shell UI by going to 
REM  http://localhost:4040/environment/ and checking if the spark.serializer property is set.
REM  By default Kryo Serialization is not enabled. 
REM  In this exercise we will walk through the process of enabling Kryo Serialization for the spark shell.
REM
REM  Set the Serialization setting in "spark-shell.cmd" file.
REM
REM  set SPARK_JAVA_OPTS=%SPARK_JAVA_OPTS% -Dspark.serializer=org.apache.spark.serializer.KryoSerializer -Dspark.kryo.registrator=org.apache.spark.graphx.GraphKryoRegistrator

REM
REM  Four different ways to run Spark Shell.
REM 

REM
REM  OPTION# 1: run Spark locally with one worker thread (no parallelism) 
REM
spark-shell.cmd
or
spark-shell.cmd local


REM
REM  OPTION# 2: run Spark locally with K worker threads (ideally set to # of cores)
REM
spark-shell.cmd local[2]

REM
REM  OPTION# 3: connect to a Spark standalone cluster.
REM  spark-shell.cmd spark://HOST:PORT
REM  Example: MASTER=spark://localhost:7077
REM
spark-shell.cmd MASTER=spark://localhost:7077

REM
REM  OPTION# 4: connect to a Mesos cluster.
REM  spark-shell.cmd mesos://HOST:PORT
REM

REM
REM  Check the Spark Context variable.
REM

sc

sc.version

sc.appName

---------------------------------------------------------------------------------------------------------
// Spark UI:
http://localhost:4040
---------------------------------------------------------------------------------------------------------

// Spark API Import Statements.
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD

// Load the article data from text file into RDD.
val articles: RDD[String] = sc.textFile("graphx/data/graphx-wiki-vertices.txt")

// Display the count of the article dataset:
articles.count

// Display the title of the first article:
articles.first

// Take a look at the transformed RDD operator graph
articles.toDebugString

// Output:
// res0: String = 6598434222544540151	Adelaide Hanscom Leeson


// Load the links data from text file into RDD.
val links: RDD[String] = sc.textFile("graphx/data/graphx-wiki-edges.txt")

links.count

links.first


// Construct the Graph
// Use the articles and links to construct a graph of Berkeley-related articles.

// Parse the article rows into pairs of vertex ID and title.
val vertices = articles.map { line =>
	val fields = line.split('\t')
	(fields(0).toLong, fields(1))
}

vertices.first


// Parse the link rows into Edge objects with the placeholder 0 attribute:
val edges = links.map { line =>
	val fields = line.split('\t')
	Edge(fields(0).toLong, fields(1).toLong, 0)
}

edges.first


// Create the graph by calling the Graph constructor with our vertex RDD, our edge RDD, and a default vertex attribute.
// Also cache the resulting graph in memory to avoid reloading it from disk each time we use it.
val graph = Graph(vertices, edges, "").cache()

// Force the graph to be computed by counting how many articles it has.
graph.vertices.count


// Computing the triplets will require an additional join but this should run quickly now that the indexes have been created.
graph.triplets.count

// Let’s look at the first few triplets:
graph.triplets.take(5).foreach(println(_))

// ((146271392968588,Computer Consoles Inc.),(7097126743572404313,Berkeley Software Distribution),0)
// ((146271392968588,Computer Consoles Inc.),(8830299306937918434,University of California, Berkeley),0)
// ((1889887370673623,Anthony Pawson),(8830299306937918434,University of California, Berkeley),0)
// ((1889887578123422,Anthony Wilden),(6990487747244935452,Busby Berkeley),0)
// ((3044656966074398,Pacific Boychoir),(8262690695090170653,Uc berkeley),0)

// Every triplet in this dataset mentions Berkeley either in the source or the destination article title.

// Running PageRank on Wikipedia


// Do some actual graph analytics. 
// For this example, we are going to run PageRank to evaluate what the most important pages in the Wikipedia graph are.
val prGraph = graph.pageRank(0.001).cache()

// Graph.pageRank returns a graph whose vertex attributes are the PageRank values of each page.
// The 0.001 parameter is the error tolerance that tells PageRank when the ranks have converged.


// Find the titles of the ten most important articles in the Berkeley subgraph of Wikipedia.
val titleAndPrGraph = graph.outerJoinVertices(prGraph.vertices) {
	(v, title, rank) => (rank.getOrElse(0.0), title)
}
titleAndPrGraph.vertices.top(10) {
	Ordering.by((entry: (VertexId, (Double, String))) => entry._2._1)
}.foreach(t => println(t._2._2 + ": " + t._2._1))


---------------------------------------------------------------------------------------------------------

//
import org.apache.spark.graphx.lib._
val prGraph = PageRank.run(graph, 100)
//

---------------------------------------------------------------------------------------------------------

